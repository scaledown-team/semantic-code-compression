{"context": "--- START OF FILE: main.py ---\n\nfrom fastapi import FastAPI, APIRouter, Request\nfrom knowledge_base.setup import lifespan_event_handler\nimport uvicorn\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom dotenv import load_dotenv\nimport os\n\nfrom api.v1.model import router as model_router\n\napp = FastAPI(lifespan=lifespan_event_handler)\n\n# Load environment variables from .env file\nload_dotenv()\n\norigins = [\n    os.getenv(\"FRONTEND_URL\")\n]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(model_router, prefix=\"/api/v1/model\")\n\n@app.get(\"/health\")\nasync def health_check(request: Request):\n    \"\"\"\n    Health check endpoint to verify if the API is running.\n    \"\"\"\n    \n    # testing vector store and query engine\n    vector_store = request.app.state.get(\"vector_store\")\n    if vector_store is None:\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"Vector store not initialized\"}, status_code=503)\n\n    query_engine = request.app.state.get(\"query_engine\")\n    if query_engine is None:\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"Query engine not initialized\"}, status_code=503)\n\n    return JSONResponse(content={\"status\": \"ok\"}, status_code=200)\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n--- END OF FILE: main.py ---\n\n--- START OF FILE: database/db.py ---\n\nfrom sqlmodel import create_engine\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(override=True)\n\nENV = os.getenv(\"ENVIRONMENT\", \"dev\").lower()\n\nsql_url = \"\"\nif ENV == \"dev\":\n    sqlite_file_name = \"database.db\"\n    sql_url = f\"sqlite:///{sqlite_file_name}\"\n\nelif ENV == \"prod\":\n    db_uri = os.getenv(\"DATABASE_URI\")\n    if not db_uri:\n        raise ValueError(\"DATABASE_URI must be set in environment variables for production.\")\n    sql_url = db_uri\n\nengine = create_engine(sql_url)\n\n--- END OF FILE: database/db.py ---\n\n--- START OF FILE: tools/agent_tools.py ---\n\n\nfrom utils.data_extraction import extract_metadata_filters_from_query\nfrom llama_index.core.schema import Document, NodeWithScore\nfrom llama_index.core.vector_stores import MetadataFilters, FilterOperator, ExactMatchFilter\nfrom knowledge_base.core import logger\nfrom typing import List, Dict, Any\nfrom pydantic_ai import RunContext\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom tavily import TavilyClient\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables\nload_dotenv()\nTAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n\ntavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n\n\nasync def get_information_from_db(query_text: str, index: VectorStoreIndex, top_k: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"\n    Get information from the knowledge base by querying relevant documents.\n    \"\"\"\n    logger.info(f\"--- Calling Vector Database for query: {query_text} ---\")\n    retriever = index.as_retriever(similarity_top_k=top_k)\n    retrieved_nodes = await retriever.aretrieve(query_text)\n    \n    documents = [\n        {\"text\": node.get_content(), \"metadata\": node.metadata or {}}\n        for node in retrieved_nodes\n    ]\n    logger.info(f\"Retrieved {len(documents)} documents from DB.\")\n    return documents\n\nasync def search_internet(query: str) -> str:\n    \"\"\"\n    Searches the internet using Tavily and returns a formatted string of results.\n    \"\"\"\n    # replace \"\" with blank spaces in the query\n    query = query.replace('\"', '').strip()\n    logger.info(f\"--- Calling Internet Search with optimized query: {query} ---\")\n    try:\n        response = tavily_client.search(query=query, num_results=5, safe_search=True, )\n    except Exception as e:\n        logger.error(f\"Tavily API call failed: {e}\")\n        return \"Internet search failed.\"\n\n    # Process the response to create a clean context string\n    if not response or not response.get(\"results\"):\n        return \"No results found from internet search.\"\n    \n    # Format the results into a single string for the LLM\n    context_str = \"Internet Search Results:\\n\"\n    for result in response[\"results\"]:\n        context_str += f\"- URL: {result.get('url')}\\n\"\n        context_str += f\"  Title: {result.get('title')}\\n\"\n        context_str += f\"  Content: {result.get('content')}\\n\\n\"\n        \n    return context_str\n\n--- END OF FILE: tools/agent_tools.py ---\n\n--- START OF FILE: utils/prompts.py ---\n\n\n# BASE PROMPT TEMPLATE\n# This is the core instruction set for the agent's persona, role, and tone.\n\nBASE_PROMPT = \"\"\"\nYou are Fair Digital Kazi GPT, an AI assistant created by Pollicy to promote fair, inclusive digital labour practices across Africa.\n\n# ROLE & FOCUS\n\u2022 Serve only questions about the digital labour platform economy in Africa.\n\u2022 Do not generate code under any circumstance.\n\u2022 If the user asks for off-topic requests (e.g., code generation, image creation), politely and casually state that you cannot help with that.\n\n# TONE & STYLE\n\u2022 Answer directly. Do not preface your response with \"based on the given context\" or similar phrases.\n\u2022 For general questions about your identity (\"Who are you?\", \"What can you do?\") or about Pollicy, introduce yourself and explain your purpose.\n\u2022 If a user's query is on-topic but cannot be answered by the provided context, state that you are still learning and don't have enough information on that specific topic.\n\n{content_sourcing_and_citation}\n\n# POLLICY OVERVIEW\n\u2022 Pollicy is a feminist organization advancing data, technology, and design for social impact in Africa (www.pollicy.org).\n\n# PRIMARY FUNCTIONS\nYou are designed to:\n1. Define and explain key concepts (e.g., worker, gig work, digital labour) in the African context.\n2. Summarize legal frameworks, market conditions, and stakeholder roles across African countries.\n3. Recommend evidence-based protections (fair wages, social security, data rights, algorithmic transparency).\n4. Highlight gendered and intersectional dimensions of platform work.\n5. Support advocates, policymakers, researchers, and workers with clear, empathetic guidance.\n\"\"\"\n\n# CITATION INSTRUCTIONS\n# This block defines the strict rules for sourcing and citing information.\n\nCITATION_INSTRUCTIONS = \"\"\"\n# CONTENT SOURCING & CITATION\n\u2022 You MUST use only the documents provided in the context to answer the user's question.\n\u2022 For **every factual statement or sentence**, you MUST append one or more citations **at the end of that sentence**.\n\u2022 The citation format is EXACTLY: [## url_link, title, page_label ##]\n\u2022 If multiple sources support a single statement, separate them with commas inside the brackets: ...statement... [## url1, title1, p1 ##], [## url2, title2, p2 ##]\n\u2022 If a source does not have a page label, use 0 by default. A citation must always have three parts.\n\u2022 DO NOT invent sources or cite anything that is not explicitly provided in the context.\n\u2022 If the title includes a comma, replace it with a semicolon to avoid confusion in the citation format. The overall number of commas must be THREE in each citation.\n\n# Sample Citation Usage:\n\u2022 The average gig worker in Nairobi earns USD 5\u20137 per day [## https://example.com/labor_survey.pdf, Gig Workers in East Africa, 12 ##].\n\u2022 Pollicy was founded in 2015 [## https://example.com/pollicy_brochure.pdf, About Pollicy, 1 ##], [## https://example.com/pollicy_annual_report.pdf, Our History, 3 ##].\n\"\"\"\n\n# Used for greetings, identity questions, and off-topic requests.\nGENERAL_BOT_PROMPT = BASE_PROMPT.format(content_sourcing_and_citation=\"\")\n\n\n# This is the main prompt for answering questions using retrieved documents.\n# It includes the detailed citation rules and a placeholder for the context.\nCONTEXTUAL_AGENT_PROMPT = BASE_PROMPT.format(content_sourcing_and_citation=CITATION_INSTRUCTIONS) + \"\"\"\n----------------\nBased on the rules above, answer the user's question using ONLY the following context.\n\nCONTEXT:\n{context}\n\"\"\"\n\n# CLASSIFICATION & GRADING PROMPTS\n# These prompts are for the internal decision-making steps of the agent.\n\nCLASSIFIER_PROMPT = \"\"\"\nYou are a master at classifying user intent based on the rules of the Fair Digital Kazi GPT. Classify the user's message into one of two categories:\n\n1.  **general_question**:\n    *   Greetings (\"Hello\", \"How are you?\").\n    *   Questions about your identity (\"Who are you?\", \"Who made you?\").\n    *   Direct questions about your creator (\"What is Pollicy?\").\n    *   Off-topic requests (asking for code, poems, jokes, etc.).\n\n2.  **context_needed**:\n    *   Any question related to the digital labour economy, platform work, gig workers, legal frameworks, worker rights, fair wages, data rights, algorithmic transparency, or intersectional dimensions of this work in Africa.\n    *   Questions that would require you to perform one of your PRIMARY FUNCTIONS.\n\nUser Message:\n\"{question}\"\n\"\"\"\n\n# --- Prompt for the document grader node ---\nGRADER_PROMPT = \"\"\"\nYou are a meticulous grader. Your task is to evaluate if the retrieved documents contain sufficient information to write a high-quality, well-cited answer that fulfills the user's request, according to the standards of the Fair Digital Kazi GPT.\n\nThe AI assistant's primary functions are:\n1. Define and explain key concepts.\n2. Summarize legal frameworks and market conditions.\n3. Recommend evidence-based protections.\n4. Highlight gendered and intersectional dimensions.\n5. Provide clear, empathetic guidance.\n\nNow, evaluate the following documents against the user's question.\n\n**User Question**: \"{question}\"\n\n**Retrieved Documents**:\n{documents}\n\n**Your Decision**:\nAre these documents sufficient to construct a comprehensive and well-cited answer that directly addresses the user's question and aligns with the assistant's primary functions? Provide a binary score and your reasoning.\n\"\"\"\n\nREWRITE_QUERY_PROMPT = \"\"\"\nYou are an expert at rewriting a user's conversational question into a concise, effective search query for a web search engine like Tavily.\nYour goal is to extract the core informational intent from the user's question, removing any conversational filler, personal anecdotes, or preamble.\n\n**User's Original Question**:\n\"{question}\"\n\nBased on this question, generate a single, optimized search query that is most likely to return relevant documents about digital labour, gig work, or fair work practices in Africa.\n\"\"\"\n\n--- END OF FILE: utils/prompts.py ---\n\n--- START OF FILE: utils/embeddings.py ---\n\n# import sentence_transformers\n\n# # model = sentence_transformers.SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n\n\n# def embed_text(text: str) -> list[float]:\n#     \"\"\"\n#     Embed a given text using the Nomic AI embedding model.\n\n#     Args:\n#         text (str): The text to embed.\n\n#     Returns:\n#         list[float]: The embedding vector for the text.\n#     \"\"\"\n#     return model.encode(text, convert_to_tensor=True).tolist()\n\n--- END OF FILE: utils/embeddings.py ---\n\n--- START OF FILE: utils/chuck_document.py ---\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n\ndef chunk_document(document: str, chunk_size: int = 768, chunk_overlap: int = 200) -> list[str]:\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    return text_splitter.split_text(document)\n\n--- END OF FILE: utils/chuck_document.py ---\n\n--- START OF FILE: utils/data_extraction.py ---\n\nimport json\nfrom typing import Dict, Any\nimport asyncio\nfrom pydantic import BaseModel\nfrom llama_index.core import PromptTemplate\nfrom schemas.query_schemas import DocumentMetadata\n\nMETADATA_EXTRACTION_PROMPT = \"\"\"\nAnalyze the following user query to identify any specific constraints related to document metadata.\nExtract values for the following fields if mentioned:\n- year (as a number, e.g., 2023)\n- country (as a string, e.g., Germany)\n- document_type (as a string, e.g., Legal Framework, report, Policy Brief, Legal Paper)\n- sub_topic (as a string)\n- title (as a string)\n- sub_category (as a string)\n\nIgnore fields not mentioned or unclear values.\nIf a field is mentioned, provide its value. If multiple values for the same field are mentioned (e.g., \"reports from 2022 or 2023\"), try to extract the first clear value or indicate ambiguity if necessary (though simple equality is assumed).\nIf no metadata constraints are found, return an empty JSON object.\n\nOutput the extracted metadata as a JSON object. Do NOT include any other text or formatting outside the JSON.\n\nExample Queries and Expected Output:\nQuery: Tell me about reports from 2022 in Germany.\nOutput: {{\"year\": 2022, \"country\": \"Germany\", \"document_type\": \"report\"}}\n\nQuery: What policies were released in 2023?\nOutput: {{\"year\": 2023, \"document_type\": \"policy\"}}\n\nQuery: Find documents about sustainable agriculture.\nOutput: {{}}\n\nQuery: Show me the document titled 'Annual Report'.\nOutput: {{\"title\": \"Annual Report\"}}\n\nQuery: {query_str}\nOutput:\n\"\"\"\n\nmetadata_extraction_template = PromptTemplate(METADATA_EXTRACTION_PROMPT)\n\nasync def extract_metadata_filters_from_query(query_text: str, llm, logger) -> Dict[str, Any]:\n    \"\"\"Uses LLM to extract metadata filters from a natural language query.\"\"\"\n    logger.info(f\"Attempting to extract metadata filters from query: '{query_text}'\")\n    try:\n        # Use the configured LLM to generate the extraction prompt\n        prompt = metadata_extraction_template.format(query_str=query_text)\n\n        # Call the LLM\n        response = llm.complete(prompt)\n        llm_output = response.text.strip()\n\n        logger.info(f\"LLM extraction raw output: {llm_output}\")\n\n        # Attempt to parse the JSON output\n        # Sometimes LLMs might add extra text, try to find the JSON part\n        try:\n            json_start = llm_output.find('{')\n            json_end = llm_output.rfind('}') + 1\n            if json_start != -1 and json_end != -1:\n                 json_string = llm_output[json_start:json_end]\n                 extracted_filters = json.loads(json_string)\n            else:\n                 logger.warning(\"Could not find JSON object in LLM output.\")\n                 extracted_filters = {} # Return empty if no JSON found\n\n\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON from LLM output: {e}. Output: {llm_output}\", exc_info=True)\n            extracted_filters = {} # Return empty on JSON error\n        except Exception as e:\n            logger.error(f\"Unexpected error parsing LLM output: {e}. Output: {llm_output}\", exc_info=True)\n            extracted_filters = {} # Return empty on other parsing errors\n\n        # Basic validation against the DocumentMetadata model keys\n        valid_filters = {}\n        allowed_keys = DocumentMetadata.__fields__.keys()\n        for key, value in extracted_filters.items():\n            if key in allowed_keys and value is not None: # Only include keys defined in our model\n                 valid_filters[key] = value\n            else:\n                logger.warning(f\"LLM extracted potentially invalid filter key/value: {key}: {value}. Ignoring.\")\n\n        logger.info(f\"Extracted and validated filters: {valid_filters}\")\n        return valid_filters\n\n    except Exception as e:\n        logger.error(f\"An error occurred during LLM metadata extraction: {e}\", exc_info=True)\n        return {} # Return empty filters on any error\n\n\n--- END OF FILE: utils/data_extraction.py ---\n\n--- START OF FILE: utils/document_reader.py ---\n\nfrom langchain_community.document_loaders import PyPDFLoader, UnstructuredPowerPointLoader, Docx2txtLoader\n\ndef read_pdf(file_path: str) -> str:\n    \"\"\"\n    Reads a PDF file and extracts text from it.\n    \"\"\"\n    loader = PyPDFLoader(file_path)\n    documents = loader.load()\n    return \"\\n\".join([doc.page_content for doc in documents])\n\ndef read_powerpoint(file_path: str) -> str:\n    \"\"\"\n    Reads a PowerPoint file and extracts text from it.\n    \"\"\"\n    loader = UnstructuredPowerPointLoader(file_path)\n    documents = loader.load()\n    return \"\\n\".join([doc.page_content for doc in documents])\n\ndef read_docx(file_path: str) -> str:\n    \"\"\"\n    Reads a DOCX file and extracts text from it.\n    \"\"\"\n    loader = Docx2txtLoader(file_path)\n    documents = loader.load()\n    return \"\\n\".join([doc.page_content for doc in documents])\n\n\n--- END OF FILE: utils/document_reader.py ---\n\n--- START OF FILE: models/file_tables.py ---\n\nfrom datetime import datetime\nfrom sqlmodel import Field, SQLModel\n\nclass FileTable(SQLModel, table=True):\n    id: int = Field(default=None, primary_key=True)\n    file_title: str\n    file_path: str | None = Field(default=None)\n    file_source: str\n    uploaded_at: datetime = Field(default_factory=datetime.utcnow)\n\n--- END OF FILE: models/file_tables.py ---\n\n--- START OF FILE: models/documents_table.py ---\n\nfrom datetime import datetime\nfrom sqlmodel import Field, SQLModel\n\nclass DocumentTable(SQLModel, table=True):\n    id: int = Field(default=None, primary_key=True)\n    document_str_id: str\n    file_table_id: int = Field(foreign_key=\"filetable.id\")\n\n--- END OF FILE: models/documents_table.py ---\n\n--- START OF FILE: schemas/query_schemas.py ---\n\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass QueryRequest(BaseModel):\n    query: str\n    \n    \nclass DocumentMetadata(BaseModel):\n    title: str\n    year: Optional[int] = None\n    document_type: Optional[str] = None\n    sub_topic: Optional[str] = None\n    title: Optional[str] = None\n    sub_category: Optional[str] = None\n    country: Optional[str] = None\n    file_source: Optional[str] = None\n    \nclass Source(BaseModel):\n    file_name: str\n    page_label: int\n    document_metadata: DocumentMetadata\n\n--- END OF FILE: schemas/query_schemas.py ---\n\n--- START OF FILE: knowledge_base/core.py ---\n\nimport os\nimport logging\nimport shutil\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\n# LlamaIndex Imports\nfrom llama_index.core import (\n    VectorStoreIndex,\n    SimpleDirectoryReader,\n    StorageContext,\n    Settings,\n    load_index_from_storage\n)\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.groq import Groq\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nENVIRONMENT = os.getenv(\"ENVIRONMENT\", \"dev\").lower()\n\n# --- Configuration ---\nDATA_DIR = Path(\"uploaded_files\")\nINDEX_STORAGE_DIR = Path(\"index_storage_chroma\") # Renamed for clarity\nCHROMA_COLLECTION_NAME = \"knowledge_base_collection\"\nEMBED_MODEL = \"BAAI/bge-small-en-v1.5\"\nGROQ_MODEL = \"llama3-8b-8192\"\nCHUNK_SIZE = 512\nCHUNK_OVERLAP = 50\n\n# --- FastAPI App State Keys ---\nINDEX_KEY = \"llama_index\"\nQUERY_ENGINE_KEY = \"query_engine\"\nVECTOR_STORE_KEY = \"vector_store\"\n\n\ndef setup_llama_index_components():\n    \"\"\"\n    Sets up LlamaIndex components based on the ENVIRONMENT variable.\n    - 'dev': Uses a persistent local ChromaDB vector store.\n    - 'prod': Uses a cloud-based Pinecone vector store.\n\n    For both environments, if the index is empty or doesn't exist, it's\n    created from documents in the DATA_DIR.\n    \"\"\"\n    logger.info(f\"Setting up LlamaIndex components in '{ENVIRONMENT}' mode.\")\n\n    if not GROQ_API_KEY:\n        logger.error(\"GROQ_API_KEY not found in environment variables.\")\n        raise ValueError(\"GROQ_API_KEY must be set.\")\n\n    # 1. Configure global LlamaIndex Settings\n    try:\n        Settings.llm = Groq(model=GROQ_MODEL, api_key=GROQ_API_KEY)\n        Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)\n        Settings.chunk_size = CHUNK_SIZE\n        Settings.chunk_overlap = CHUNK_OVERLAP\n        logger.info(\"Global LlamaIndex Settings configured.\")\n    except Exception as e:\n        logger.error(f\"Failed to configure LlamaIndex Settings: {e}\", exc_info=True)\n        raise\n\n    index = None\n    vector_store = None\n\n    # 2. Setup Vector Store and Index based on environment\n    if ENVIRONMENT == 'prod':\n        # --- PRODUCTION: PINECONE ---\n        logger.info(\"Using Pinecone vector store for production.\")\n        from llama_index.vector_stores.pinecone import PineconeVectorStore\n        from pinecone import Pinecone, ServerlessSpec\n\n        api_key = os.getenv(\"PINECONE_API_KEY\")\n        index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n\n        if not api_key or not index_name:\n            raise ValueError(\"PINECONE_API_KEY and PINECONE_INDEX_NAME must be set for prod environment.\")\n\n        try:\n            pc = Pinecone(api_key=api_key)\n            \n            # Dynamically get embedding dimension\n            embed_dim = len(Settings.embed_model.get_text_embedding(\"test query\"))\n\n            # Check if the Pinecone index exists, create if it doesn't\n            if index_name not in pc.list_indexes().names():\n                logger.warning(f\"Pinecone index '{index_name}' not found. Creating a new one...\")\n                pc.create_index(\n                    name=index_name,\n                    dimension=embed_dim,\n                    metric=\"cosine\", # Cosine similarity is common for text embeddings\n                    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n                )\n                logger.info(f\"Pinecone index '{index_name}' created successfully.\")\n\n            pinecone_index = pc.Index(index_name)\n            vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n            \n            # Check if index is populated to decide whether to build or load\n            stats = pinecone_index.describe_index_stats()\n            if stats['total_vector_count'] > 0:\n                logger.info(f\"Connecting to existing, populated Pinecone index '{index_name}'.\")\n                index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n            else:\n                logger.info(f\"Pinecone index '{index_name}' is empty. Index will be built from documents.\")\n                # 'index' remains None, triggering the build process below\n        except Exception as e:\n            logger.error(f\"Failed to initialize Pinecone: {e}\", exc_info=True)\n            raise RuntimeError(\"Could not connect to or create Pinecone index.\") from e\n\n    else:\n        # --- DEVELOPMENT: CHROMA DB ---\n        logger.info(\"Using ChromaDB vector store for development.\")\n        import chromadb\n        from llama_index.vector_stores.chroma import ChromaVectorStore\n        \n        INDEX_STORAGE_DIR.mkdir(parents=True, exist_ok=True)\n        try:\n            db = chromadb.PersistentClient(path=str(INDEX_STORAGE_DIR))\n            collection = db.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n            vector_store = ChromaVectorStore(chroma_collection=collection)\n            \n            storage_context = StorageContext.from_defaults(\n                persist_dir=str(INDEX_STORAGE_DIR),\n                vector_store=vector_store\n            )\n            index = load_index_from_storage(storage_context=storage_context)\n            logger.info(\"ChromaDB index loaded successfully from disk.\")\n        except Exception:\n            logger.warning(f\"Could not load index from {INDEX_STORAGE_DIR}. It might not exist yet. A new one will be created.\")\n\n    # 3. Build a new index if it wasn't loaded\n    if index is None:\n        logger.info(\"No existing index found. Building a new one from documents...\")\n        try:\n            DATA_DIR.mkdir(parents=True, exist_ok=True)\n            \n            # First, create the storage context, as it's needed for both empty and populated indices\n            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n            \n            if not any(DATA_DIR.iterdir()):\n                logger.info(f\"No documents found in {DATA_DIR}. Creating an empty index.\")\n            \n                # If there are no documents, create an empty index\n                logger.warning(f\"No documents found in {DATA_DIR}. Creating an empty but valid index.\")\n                index = VectorStoreIndex([], storage_context=storage_context)\n            else:\n                # If documents are found, build the index from them\n                documents = SimpleDirectoryReader(str(DATA_DIR)).load_data()\n\n                logger.info(f\"Loaded {len(documents)} document(s). Building index...\")\n                index = VectorStoreIndex.from_documents(\n                    documents,\n                    storage_context=storage_context,\n                    show_progress=True\n                )\n            \n            logger.info(\"New index created successfully.\")\n\n            # For ChromaDB, persist the index metadata to disk.\n            # This is also important for an empty index so it can be loaded next time.\n            if ENVIRONMENT == 'dev':\n                logger.info(f\"Persisting new ChromaDB index structure to {INDEX_STORAGE_DIR}...\")\n                index.storage_context.persist(persist_dir=str(INDEX_STORAGE_DIR))\n                logger.info(\"ChromaDB index persisted.\")\n\n        except Exception as e:\n            logger.error(f\"Failed to create index: {e}\", exc_info=True)\n            raise RuntimeError(\"Fatal error: Index creation failed.\") from e\n\n    return index, vector_store\n\n--- END OF FILE: knowledge_base/core.py ---\n\n--- START OF FILE: knowledge_base/setup.py ---\n\n\nfrom fastapi import FastAPI, Request, HTTPException, UploadFile, File, Depends\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport contextlib\nfrom .core import setup_llama_index_components, logger, INDEX_KEY, QUERY_ENGINE_KEY\nfrom database.db import engine\nfrom models.file_tables import *\nfrom models.documents_table import *\nfrom sqlmodel import SQLModel\n\n# --- FastAPI Lifespan ---\n\n@contextlib.asynccontextmanager\nasync def lifespan_event_handler(app: FastAPI):\n    \"\"\"\n    FastAPI lifespan. Sets up LlamaIndex and stores index/query engine in app state.\n    \"\"\"\n    print(\"Application startup begins.\")\n\n    # --- Startup: Setup LlamaIndex and Query Engine ---\n    index = None\n    vector_store = None\n    try:\n        index, vector_store = setup_llama_index_components()\n        if index and vector_store:\n            # Store the index and vector store in the application state\n            app.state.llama_index = index\n            app.state.vector_store = vector_store \n\n            logger.info(\"Creating initial query engine...\")\n            app.state.query_engine = index.as_query_engine(similarity_top_k=3)\n            logger.info(\"Initial query engine created and stored in app state.\")\n\n        else:\n             logger.error(\"LlamaIndex setup failed. Index or vector store not available.\")\n             raise RuntimeError(\"Failed to initialize LlamaIndex components.\")\n         \n        \n        # Initialize the database engine\n        logger.info(\"Initializing database engine...\")\n        SQLModel.metadata.create_all(engine)\n        logger.info(\"Database engine initialized and tables created if not exist.\")\n         \n\n    except Exception as e:\n        logger.critical(f\"Critical error during LlamaIndex setup: {e}\", exc_info=True)\n        raise RuntimeError(\"Application startup failed due to LlamaIndex setup error.\") from e\n\n    # --- Startup successful ---\n    print(\"Application startup complete. Yielding control.\")\n    yield # Application starts serving requests now\n    \n    print(\"Application shutdown complete.\")\n\n\n\n--- END OF FILE: knowledge_base/setup.py ---\n\n--- START OF FILE: knowledge_base/langgraph_init.py ---\n\nfrom dotenv import load_dotenv\nfrom typing import Annotated, Literal, List\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\nfrom tools.agent_tools import get_information_from_db, search_internet\nfrom utils import prompts\nfrom langchain_core.runnables import RunnableConfig\n\n\nload_dotenv()\n\nllm = init_chat_model(\n    \"google_genai:gemini-2.0-flash\"\n)\n\n# --- 1. State Definition ---\nclass GraphState(TypedDict):\n    messages: Annotated[list, add_messages]\n    question: str\n    documents: List[str]\n    message_type: str\n    retrieval_status: str\n    rewritten_question: str\n\n# --- 2. Pydantic Models for LLM Decisions ---\nclass MessageClassifier(BaseModel):\n    \"\"\"Classify the user's message.\"\"\"\n    message_type: Literal[\"general_question\", \"context_needed\"] = Field(\n        description=\"The category of the user's message.\"\n    )\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for document relevance.\"\"\"\n    is_sufficient: bool = Field(description=\"Are the documents sufficient to answer the question?\")\n    reasoning: str = Field(description=\"A brief explanation for the decision.\")\n\n# --- 3. Graph Nodes ---\n\nasync def classify_message(state: GraphState):\n    last_message = state[\"messages\"][-1]\n    question = last_message.content\n    classifier = llm.with_structured_output(MessageClassifier)\n    \n    result = await classifier.ainvoke([\n        {\"role\": \"system\", \"content\": prompts.CLASSIFIER_PROMPT},\n        {\"role\": \"user\", \"content\": question}\n    ])\n    \n    return {\"message_type\": result.message_type, \"question\": question}\n\nasync def general_agent(state: GraphState):\n    response = await llm.ainvoke([\n        {\"role\": \"system\", \"content\": prompts.GENERAL_BOT_PROMPT},\n        {\"role\": \"user\", \"content\": state[\"question\"]}\n    ])\n    return {\"messages\": [response]}\n\nasync def retrieve_from_db(state: GraphState, config: RunnableConfig):\n    question = state[\"question\"]\n    index = config[\"configurable\"][\"llama_index\"]\n    \n    docs = await get_information_from_db(query_text=question, index=index)\n    print(docs)\n\n    formatted_docs = [f\"Title: {doc['metadata'].get('title', 'N/A')} \\nSource: {doc['metadata'].get('file_source', 'N/A')}\\nContent: {doc['text']}\" for doc in docs]\n    return {\"documents\": formatted_docs}\n\nasync def grade_retrieval(state: GraphState):\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    \n    if not documents:\n        return {\"retrieval_status\": \"insufficient\"}\n\n    grader = llm.with_structured_output(GradeDocuments)\n    task = f\"User Question: {question}\\n\\nRetrieved Documents:\\n{''.join(documents)}\"\n    result = await grader.ainvoke([\n        {\"role\": \"system\", \"content\": prompts.GRADER_PROMPT},\n        {\"role\": \"user\", \"content\": task}\n    ])\n    \n    status = \"sufficient\" if result.is_sufficient else \"insufficient\"\n    return {\"retrieval_status\": status}\n\nasync def rewrite_query(state: GraphState):\n    \"\"\"\n    Rewrites the user's question into an effective search query.\n    \"\"\"\n    print(\"--- Rewriting Query for Web Search ---\")\n    question = state[\"question\"]\n    \n    prompt = prompts.REWRITE_QUERY_PROMPT.format(question=question)\n    \n    # Use the LLM to get the rewritten query\n    rewriter_llm = llm.with_config(tags=[\"rewriter\"])\n    rewritten_query = await rewriter_llm.ainvoke(prompt)\n    \n    print(f\"Original: '{question}' | Rewritten: '{rewritten_query.content}'\")\n    \n    return {\"rewritten_question\": rewritten_query.content}\n\nasync def search_the_web(state: GraphState):\n    \"\"\"\n    Performs an internet search using the REWRITTEN query.\n    \"\"\"\n    rewritten_query = state[\"rewritten_question\"]\n    web_results = await search_internet(query=rewritten_query)\n    \n    all_docs = state.get(\"documents\", []) + [web_results]\n    return {\"documents\": all_docs}\n\nasync def generate_answer(state: GraphState):\n    question = state[\"question\"]\n    context_str = \"\\n\\n\".join(state[\"documents\"])\n    \n    prompt = prompts.CONTEXTUAL_AGENT_PROMPT.format(context=context_str)\n    response = await llm.ainvoke([\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": question}\n    ])\n    return {\"messages\": [response]}\n\n\n# --- 4. Graph Definition and Compilation ---\ndef create_agent_graph():\n    \"\"\"Factory function to create the LangGraph agent.\"\"\"\n    workflow = StateGraph(GraphState)\n\n    # Add nodes (including the new one)\n    workflow.add_node(\"classifier\", classify_message)\n    workflow.add_node(\"general_agent\", general_agent)\n    workflow.add_node(\"retrieve_from_db\", retrieve_from_db)\n    workflow.add_node(\"grade_retrieval\", grade_retrieval)\n    workflow.add_node(\"rewrite_query\", rewrite_query) # <-- New node\n    workflow.add_node(\"internet_search\", search_the_web) # This is your updated search node\n    workflow.add_node(\"generate_answer\", generate_answer)\n\n    # Add edges\n    workflow.set_entry_point(\"classifier\")\n    workflow.add_conditional_edges(\n        \"classifier\",\n        lambda state: state[\"message_type\"],\n        {\"general_question\": \"general_agent\", \"context_needed\": \"retrieve_from_db\"}\n    )\n    \n    # *** THIS IS THE KEY LOGIC CHANGE ***\n    workflow.add_conditional_edges(\n        \"grade_retrieval\",\n        lambda state: state[\"retrieval_status\"],\n        {\n            \"sufficient\": \"generate_answer\",\n            \"insufficient\": \"rewrite_query\"\n        }\n    )\n    \n    workflow.add_edge(\"retrieve_from_db\", \"grade_retrieval\")\n    workflow.add_edge(\"rewrite_query\", \"internet_search\")\n    workflow.add_edge(\"internet_search\", \"generate_answer\")\n    workflow.add_edge(\"general_agent\", END)\n    workflow.add_edge(\"generate_answer\", END)\n\n    return workflow.compile()\n# Create the compiled graph once when the module is loaded\nagent_graph = create_agent_graph()\n\n--- END OF FILE: knowledge_base/langgraph_init.py ---\n\n", "prompt": "Description:\n\nYou will be provided with code containing masked values represented as MASK_X, where X is a number. Your task is to predict the exact replacement text for each masked value. Respond with a JSON block within triple backticks (```json) containing a dictionary where the keys are the masked value identifiers (e.g., 'MASK_0', 'MASK_1') and the values are your predicted replacement texts. If the JSON block is not formatted or closed properly or the JSON is not valid, you will FAIL the task. Ensure your response contains only the JSON block and no additional text or explanation.\n\n\nExample Input:\n\nMasked Code:\n\n  import os\n\n  class MyProcessor:\n\n      def __init__(self, config_path):\n          self.config = self._load_config(config_path)\n\n      def _load_config(self, path):\n          if os.path.MASK_0(path):\n              with open(path, 'r') as f:\n                  data = f.read()\n              return data\n          return None\n\n      def process(self, data):\n          if data:\n              processed_data = data.upper().strip()\n              return processed_data\n          return ''\n  processor = MyProcessor('my_config.txt')\n  result = processor.MASK_1('  some DATA  ')\n  print(result)\n\nExample Output:\n\n```json\n{\"MASK_0\": \"exists\", \"MASK_1\": \"process\"}\n```\n", "code": "from fastapi import APIRouter, Depends, Request, UploadFile\nfrom fastapi.responses import JSONResponse\nimport os\nimport uuid\nfrom schemas.query_schemas import QueryRequest, Source, DocumentMetadata\nimport json\nfrom knowledge_base.core import logger\nfrom pydantic import ValidationError\nimport asyncio\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.readers.web import SimpleWebPageReader\nfrom llama_index.core.schema import Document, NodeWithScore\nfrom database.db import engine\nfrom sqlmodel import SQLModel, Session\nfrom models.file_tables import FileTable\nfrom models.documents_table import DocumentTable\nfrom typing import Annotated\nfrom knowledge_base.langgraph_init import agent_graph\nrouter = APIRouter()\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\nSessionDep = Annotated[Session, Depends(get_session)]\nUPLOADED_FILES_DIR = 'uploaded_files'\nos.MASK_0(UPLOADED_FILES_DIR, exist_ok=True)\nprint(agent_graph.get_graph().MASK_1())\n\n@router.MASK_61('/add_knowledge')\nasync def add_file_knowledge(request: Request, session: SessionDep, file: Optional[UploadFile]=None):\n    \"\"\"\n    Endpoint to add knowledge to the knowledge base.\n    File upload is optional.\n    \"\"\"\n    try:\n        form_data = await request.form()\n        documents_to_update: List[Document] = []\n        document_ids = set()\n        uploaded_file_path = None\n        print(f\"Received file: {(file.MASK_2 if file else 'None')}\")\n        if file and file.MASK_3:\n            uploaded_file_path = Path(UPLOADED_FILES_DIR) / f'{uuid.MASK_4()}_{file.MASK_5}'\n            try:\n                contents = await file.MASK_6()\n                await asyncio.MASK_7(uploaded_file_path.write_bytes, contents)\n                logger.MASK_8(f'File saved to {uploaded_file_path}')\n            except Exception as e:\n                error_msg = f'Failed to save file {file.MASK_9}: {e}'\n                logger.MASK_10(error_msg, exc_info=True)\n                return JSONResponse(content={'status': 'error', 'message': error_msg}, status_code=500)\n        if 'title' not in form_data or not form_data.MASK_11('title'):\n            return JSONResponse(content={'status': 'error', 'message': 'Title is required'}, status_code=400)\n        if 'file_source' not in form_data or not form_data.MASK_12('file_source'):\n            return JSONResponse(content={'status': 'error', 'message': 'File source is required'}, status_code=400)\n        metadata_dict = {'title': form_data.MASK_13('title'), 'file_source': form_data.MASK_14('file_source')}\n        for field in ['year', 'document_type', 'sub_topic', 'title', 'sub_category', 'country', 'file_source']:\n            if field in form_data and form_data.MASK_15(field):\n                if field == 'year' and form_data.get(field).MASK_16():\n                    metadata_dict[field] = int(form_data.MASK_17(field))\n                else:\n                    metadata_dict[field] = form_data.MASK_18(field)\n        document_metadata = DocumentMetadata(**metadata_dict)\n        extra_metadata_dict = {}\n        if document_metadata:\n            try:\n                extra_metadata_dict = document_metadata.MASK_19(exclude_none=True)\n                logger.MASK_20(f'Parsed and validated metadata: {extra_metadata_dict}')\n            except json.MASK_21:\n                error_msg = 'Metadata is not a valid JSON string.'\n                logger.MASK_22(error_msg)\n            except ValidationError as e:\n                error_msg = f'Metadata validation failed: {e.MASK_23()}'\n                logger.MASK_24(error_msg)\n            except Exception as e:\n                error_msg = f'Unexpected error parsing metadata: {e}'\n                logger.MASK_25(error_msg, exc_info=True)\n        else:\n            logger.MASK_26('No metadata provided for this file.')\n        if uploaded_file_path:\n            try:\n                docs = SimpleDirectoryReader(input_files=[str(uploaded_file_path)]).MASK_27()\n                if docs:\n                    for doc in docs:\n                        doc.metadata.MASK_28(extra_metadata_dict)\n                        documents_to_update.MASK_29(doc)\n                        document_ids.MASK_30(doc.doc_id)\n                    logger.MASK_31(f'Loaded {len(documents_to_update)} document(s) from {uploaded_file_path} with added metadata.')\n                else:\n                    warning_msg = f'No documents loaded from file {file.MASK_32}. Skipping index update.'\n                    logger.MASK_33(warning_msg)\n            except Exception as e:\n                error_msg = f'Failed to load document from {uploaded_file_path}: {e}'\n                logger.MASK_34(error_msg, exc_info=True)\n                return JSONResponse(content={'status': 'error', 'message': error_msg}, status_code=500)\n        else:\n            file_source = form_data.MASK_35('file_source')\n            try:\n                logger.MASK_36(f'Fetching documents from web source: {file_source}')\n                docs = SimpleWebPageReader(html_to_text=True).MASK_37([file_source])\n                if docs:\n                    for doc in docs:\n                        doc.metadata.MASK_38(extra_metadata_dict)\n                        documents_to_update.MASK_39(doc)\n                        document_ids.MASK_40(doc.doc_id)\n                    logger.MASK_41(f'Loaded {len(documents_to_update)} document(s) from web source {file_source} with added metadata.')\n                else:\n                    warning_msg = f'No documents loaded from web source {file_source}. Skipping index update.'\n                    logger.MASK_42(warning_msg)\n            except Exception as e:\n                error_msg = f'Failed to fetch documents from web source {file_source}: {e}'\n                logger.MASK_43(error_msg, exc_info=True)\n                return JSONResponse(content={'status': 'error', 'message': error_msg}, status_code=500)\n        try:\n            print(document_ids)\n            print(uploaded_file_path)\n            file_record = FileTable(file_title=form_data.MASK_44('title'), file_path=str(uploaded_file_path) if uploaded_file_path else None, file_source=form_data.MASK_45('file_source'))\n            session.MASK_46(file_record)\n            session.MASK_47()\n            for doc_id in document_ids:\n                document_record = DocumentTable(document_str_id=doc_id, file_table_id=file_record.MASK_48)\n                session.MASK_49(document_record)\n            session.MASK_50()\n            logger.MASK_51(f'Metadata saved to database with ID {file_record.id}.')\n        except Exception as e:\n            error_msg = f'Failed to save metadata to database: {e}'\n            logger.MASK_52(error_msg, exc_info=True)\n            return JSONResponse(content={'status': 'error', 'message': error_msg}, status_code=500)\n        index = request.app.state.MASK_53\n        try:\n            logger.MASK_54('Refreshing index with document(s) and metadata...')\n            actually_updated_docs = index.MASK_55(documents=documents_to_update, service_context=None)\n            logger.MASK_56(f'Index refresh complete. {len(actually_updated_docs)} document(s) were added or refreshed.')\n        except Exception as e:\n            error_msg = f'An error occurred during index refresh: {e}'\n            logger.MASK_57(error_msg, exc_info=True)\n            return JSONResponse(content={'status': 'error', 'message': error_msg}, status_code=500)\n        try:\n            index.storage_context.MASK_58()\n        except Exception as e:\n            error_msg = f'Failed to persist index: {e}'\n            logger.MASK_59(error_msg, exc_info=True)\n            return JSONResponse(content={'status': 'error', 'message': error_msg}, status_code=500)\n        return JSONResponse(content={'status': 'success', 'message': 'Knowledge added successfully'}, status_code=200)\n    except Exception as e:\n        import traceback\n        traceback.MASK_60()\n        return JSONResponse(content={'status': 'error', 'message': str(e)}, status_code=500)\n\n@router.MASK_69('/query')\nasync def query_knowledge(request: Request, query_request: QueryRequest):\n    \"\"\"\n    Endpoint to query the knowledge base.\n    Extracts metadata filters from the query and uses them in the vector store query.\n    Returns the response and source documents.\n    \"\"\"\n    try:\n        query_text = query_request.MASK_62\n        logger.MASK_63(f\"Received query: '{query_text}'\")\n        inputs = {'messages': [('user', query_text)]}\n        index_instance = request.app.state.MASK_64\n        config = {'configurable': {'llama_index': index_instance}}\n        final_state = await agent_graph.MASK_65(inputs, config=config)\n        response = final_state['messages'][-1].MASK_66\n        return JSONResponse(content={'response': response}, status_code=200)\n    except Exception as e:\n        import traceback\n        traceback.MASK_67()\n        logger.MASK_68(f'An error occurred during query processing: {e}', exc_info=True)\n        return JSONResponse(content={'status': 'error', 'message': 'An internal error occurred during the query.'}, status_code=500)\n\n@router.MASK_72('/files', response_model=List[FileTable])\nasync def get_uploaded_files(session: SessionDep):\n    \"\"\"\n    Get all uploaded files from the database.\n    \"\"\"\n    try:\n        files = session.query(FileTable).MASK_70()\n        return files\n    except Exception as e:\n        logger.MASK_71(f'Error fetching uploaded files: {e}', exc_info=True)\n        return JSONResponse(content={'status': 'error', 'message': 'Failed to fetch uploaded files'}, status_code=500)\n\n@router.MASK_91('/delete_file/{file_id}')\nasync def delete_file(request: Request, file_id: int, session: SessionDep):\n    \"\"\"\n    Delete a specific file by ID.\n    \"\"\"\n    try:\n        file_record = session.MASK_73(FileTable, file_id)\n        if not file_record:\n            return JSONResponse(content={'status': 'error', 'message': 'File not found'}, status_code=404)\n        index = request.app.state.MASK_74\n        logger.MASK_75(f'Attempting to delete file with ID {file_id} and associated documents from index...')\n        all_document_ids = session.query(DocumentTable).filter(DocumentTable.file_table_id == file_record.id).MASK_76()\n        if not all_document_ids:\n            logger.MASK_77(f'No associated documents found for file ID {file_id}.')\n            return JSONResponse(content={'status': 'warning', 'message': 'No associated documents found for this file'}, status_code=404)\n        for doc in all_document_ids:\n            try:\n                logger.MASK_78(f'Removing document with ID {doc.id} from index...')\n                index.MASK_79(doc.id, delete_from_docstore=True)\n                logger.info(f'Document with ID {doc.MASK_80} removed from index.')\n            except Exception as e:\n                logger.MASK_81(f'Failed to remove document with ID {doc.id} from index: {e}', exc_info=True)\n                return JSONResponse(content={'status': 'error', 'message': 'Failed to remove document from index'}, status_code=500)\n        if file_record.MASK_82 and os.path.MASK_83(file_record.file_path):\n            os.MASK_84(file_record.file_path)\n            logger.MASK_85(f'File {file_record.file_path} deleted successfully.')\n        session.query(DocumentTable).filter(DocumentTable.file_table_id == file_record.id).MASK_86()\n        session.MASK_87()\n        session.MASK_88(file_record)\n        session.MASK_89()\n        return JSONResponse(content={'status': 'success', 'message': 'File deleted successfully'}, status_code=200)\n    except Exception as e:\n        logger.MASK_90(f'Error deleting file: {e}', exc_info=True)\n        return JSONResponse(content={'status': 'error', 'message': 'Failed to delete file'}, status_code=500)", "model": "gemini-2.5-flash", "scaledown": {"rate": 0}}