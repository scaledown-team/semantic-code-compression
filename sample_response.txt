{
  "role": "bot",
  "full_response": "```json\n{\n  \"MASK_0\": \"lifespan_event_handler\",\n  \"MASK_1\": \"os.getenv(\\\"FRONTEND_URL\\\")\",\n  \"MASK_2\": \"file_title=form_data.get(\\\"title\\\"), file_path=str(uploaded_file_path) if uploaded_file_path else None, file_source=form_data.get(\\\"file_source\\\")\",\n  \"MASK_3\": \"doc.document_str_id\"\n}\n```",
  "compressed_response": "```json\n{\n  \"MASK_0\": \"lifespan_event_handler\",\n  \"MASK_1\": \"os.getenv(\\\"FRONTEND_URL\\\")\",\n  \"MASK_2\": \"file_title=form_data.get(\\\"title\\\"), file_path=str(uploaded_file_path) if uploaded_file_path else None, file_source=form_data.get(\\\"file_source\\\")\",\n  \"MASK_3\": \"doc.document_str_id\"\n}\n```",
  "compressed_prompt": "--- START OF FILE: main.py ---\n\nfrom fastapi import FastAPI, APIRouter, Request\nfrom knowledge_base.setup import lifespan_event_handler\nimport uvicorn\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom dotenv import load_dotenv\nimport os\n\nfrom api.v1.model import router as model_router\n\napp = FastAPI(lifespan=lifespan_event_handler)\n\n# Load environment variables from .env file\nload_dotenv()\n\norigins = [\n    os.getenv(\"FRONTEND_URL\")\n]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.include_router(model_router, prefix=\"/api/v1/model\")\n\n@app.get(\"/health\")\nasync def health_check(request: Request):\n    \"\"\"\n    Health check endpoint to verify if the API is running.\n    \"\"\"\n    \n    # testing vector store and query engine\n    vector_store = request.app.state.get(\"vector_store\")\n    if vector_store is None:\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"Vector store not initialized\"}, status_code=503)\n\n    query_engine = request.app.state.get(\"query_engine\")\n    if query_engine is None:\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"Query engine not initialized\"}, status_code=503)\n\n    return JSONResponse(content={\"status\": \"ok\"}, status_code=200)\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n--- END OF FILE: main.py ---\n\n--- START OF FILE: database/db.py ---\n\nfrom sqlmodel import create_engine\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv(override=True)\n\nENV = os.getenv(\"ENVIRONMENT\", \"dev\").lower()\n\nsql_url = \"\"\nif ENV == \"dev\":\n    sqlite_file_name = \"database.db\"\n    sql_url = f\"sqlite:///{sqlite_file_name}\"\n\nelif ENV == \"prod\":\n    db_uri = os.getenv(\"DATABASE_URI\")\n    if not db_uri:\n        raise ValueError(\"DATABASE_URI must be set in environment variables for production.\")\n    sql_url = db_uri\n\nengine = create_engine(sql_url)\n\n--- END OF FILE: database/db.py ---\n\n--- START OF FILE: tools/agent_tools.py ---\n\n\nfrom utils.data_extraction import extract_metadata_filters_from_query\nfrom llama_index.core.schema import Document, NodeWithScore\nfrom llama_index.core.vector_stores import MetadataFilters, FilterOperator, ExactMatchFilter\nfrom knowledge_base.core import logger\nfrom typing import List, Dict, Any\nfrom pydantic_ai import RunContext\nfrom llama_index.core import PromptTemplate\nfrom llama_index.core.indices.vector_store import VectorStoreIndex\nfrom tavily import TavilyClient\nfrom dotenv import load_dotenv\nimport os\n\n# Load environment variables\nload_dotenv()\nTAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n\ntavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n\n\nasync def get_information_from_db(query_text: str, index: VectorStoreIndex, top_k: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"\n    Get information from the knowledge base by querying relevant documents.\n    \"\"\"\n    logger.info(f\"--- Calling Vector Database for query: {query_text} ---\")\n    retriever = index.as_retriever(similarity_top_k=top_k)\n    retrieved_nodes = await retriever.aretrieve(query_text)\n    \n    documents = [\n        {\"text\": node.get_content(), \"metadata\": node.metadata or {}}\n        for node in retrieved_nodes\n    ]\n    logger.info(f\"Retrieved {len(documents)} documents from DB.\")\n    return documents\n\nasync def search_internet(query: str) -> str:\n    \"\"\"\n    Searches the internet using Tavily and returns a formatted string of results.\n    \"\"\"\n    # replace \"\" with blank spaces in the query\n    query = query.replace('\"', '').strip()\n    logger.info(f\"--- Calling Internet Search with optimized query: {query} ---\")\n    try:\n        response = tavily_client.search(query=query, num_results=5, safe_search=True, )\n    except Exception as e:\n        logger.error(f\"Tavily API call failed: {e}\")\n        return \"Internet search failed.\"\n\n    # Process the response to create a clean context string\n    if not response or not response.get(\"results\"):\n        return \"No results found from internet search.\"\n    \n    # Format the results into a single string for the LLM\n    context_str = \"Internet Search Results:\\n\"\n    for result in response[\"results\"]:\n        context_str += f\"- URL: {result.get('url')}\\n\"\n        context_str += f\"  Title: {result.get('title')}\\n\"\n        context_str += f\"  Content: {result.get('content')}\\n\\n\"\n        \n    return context_str\n\n--- END OF FILE: tools/agent_tools.py ---\n\n--- START OF FILE: utils/prompts.py ---\n\n\n# BASE PROMPT TEMPLATE\n# This is the core instruction set for the agent's persona, role, and tone.\n\nBASE_PROMPT = \"\"\"\nYou are Fair Digital Kazi GPT, an AI assistant created by Pollicy to promote fair, inclusive digital labour practices across Africa.\n\n# ROLE & FOCUS\n\u2022 Serve only questions about the digital labour platform economy in Africa.\n\u2022 Do not generate code under any circumstance.\n\u2022 If the user asks for off-topic requests (e.g., code generation, image creation), politely and casually state that you cannot help with that.\n\n# TONE & STYLE\n\u2022 Answer directly. Do not preface your response with \"based on the given context\" or similar phrases.\n\u2022 For general questions about your identity (\"Who are you?\", \"What can you do?\") or about Pollicy, introduce yourself and explain your purpose.\n\u2022 If a user's query is on-topic but cannot be answered by the provided context, state that you are still learning and don't have enough information on that specific topic.\n\n{content_sourcing_and_citation}\n\n# POLLICY OVERVIEW\n\u2022 Pollicy is a feminist organization advancing data, technology, and design for social impact in Africa (www.pollicy.org).\n\n# PRIMARY FUNCTIONS\nYou are designed to:\n1. Define and explain key concepts (e.g., worker, gig work, digital labour) in the African context.\n2. Summarize legal frameworks, market conditions, and stakeholder roles across African countries.\n3. Recommend evidence-based protections (fair wages, social security, data rights, algorithmic transparency).\n4. Highlight gendered and intersectional dimensions of platform work.\n5. Support advocates, policymakers, researchers, and workers with clear, empathetic guidance.\n\"\"\"\n\n# CITATION INSTRUCTIONS\n# This block defines the strict rules for sourcing and citing information.\n\nCITATION_INSTRUCTIONS = \"\"\"\n# CONTENT SOURCING & CITATION\n\u2022 You MUST use only the documents provided in the context to answer the user's question.\n\u2022 For **every factual statement or sentence**, you MUST append one or more citations **at the end of that sentence**.\n\u2022 The citation format is EXACTLY: [## url_link, title, page_label ##]\n\u2022 If multiple sources support a single statement, separate them with commas inside the brackets: ...statement... [## url1, title1, p1 ##], [## url2, title2, p2 ##]\n\u2022 If a source does not have a page label, use 0 by default. A citation must always have three parts.\n\u2022 DO NOT invent sources or cite anything that is not explicitly provided in the context.\n\u2022 If the title includes a comma, replace it with a semicolon to avoid confusion in the citation format. The overall number of commas must be THREE in each citation.\n\n# Sample Citation Usage:\n\u2022 The average gig worker in Nairobi earns USD 5\u20137 per day [## https://example.com/labor_survey.pdf, Gig Workers in East Africa, 12 ##].\n\u2022 Pollicy was founded in 2015 [## https://example.com/pollicy_brochure.pdf, About Pollicy, 1 ##], [## https://example.com/pollicy_annual_report.pdf, Our History, 3 ##].\n\"\"\"\n\n# Used for greetings, identity questions, and off-topic requests.\nGENERAL_BOT_PROMPT = BASE_PROMPT.format(content_sourcing_and_citation=\"\")\n\n\n# This is the main prompt for answering questions using retrieved documents.\n# It includes the detailed citation rules and a placeholder for the context.\nCONTEXTUAL_AGENT_PROMPT = BASE_PROMPT.format(content_sourcing_and_citation=CITATION_INSTRUCTIONS) + \"\"\"\n----------------\nBased on the rules above, answer the user's question using ONLY the following context.\n\nCONTEXT:\n{context}\n\"\"\"\n\n# CLASSIFICATION & GRADING PROMPTS\n# These prompts are for the internal decision-making steps of the agent.\n\nCLASSIFIER_PROMPT = \"\"\"\nYou are a master at classifying user intent based on the rules of the Fair Digital Kazi GPT. Classify the user's message into one of two categories:\n\n1.  **general_question**:\n    *   Greetings (\"Hello\", \"How are you?\").\n    *   Questions about your identity (\"Who are you?\", \"Who made you?\").\n    *   Direct questions about your creator (\"What is Pollicy?\").\n    *   Off-topic requests (asking for code, poems, jokes, etc.).\n\n2.  **context_needed**:\n    *   Any question related to the digital labour economy, platform work, gig workers, legal frameworks, worker rights, fair wages, data rights, algorithmic transparency, or intersectional dimensions of this work in Africa.\n    *   Questions that would require you to perform one of your PRIMARY FUNCTIONS.\n\nUser Message:\n\"{question}\"\n\"\"\"\n\n# --- Prompt for the document grader node ---\nGRADER_PROMPT = \"\"\"\nYou are a meticulous grader. Your task is to evaluate if the retrieved documents contain sufficient information to write a high-quality, well-cited answer that fulfills the user's request, according to the standards of the Fair Digital Kazi GPT.\n\nThe AI assistant's primary functions are:\n1. Define and explain key concepts.\n2. Summarize legal frameworks and market conditions.\n3. Recommend evidence-based protections.\n4. Highlight gendered and intersectional dimensions.\n5. Provide clear, empathetic guidance.\n\nNow, evaluate the following documents against the user's question.\n\n**User Question**: \"{question}\"\n\n**Retrieved Documents**:\n{documents}\n\n**Your Decision**:\nAre these documents sufficient to construct a comprehensive and well-cited answer that directly addresses the user's question and aligns with the assistant's primary functions? Provide a binary score and your reasoning.\n\"\"\"\n\nREWRITE_QUERY_PROMPT = \"\"\"\nYou are an expert at rewriting a user's conversational question into a concise, effective search query for a web search engine like Tavily.\nYour goal is to extract the core informational intent from the user's question, removing any conversational filler, personal anecdotes, or preamble.\n\n**User's Original Question**:\n\"{question}\"\n\nBased on this question, generate a single, optimized search query that is most likely to return relevant documents about digital labour, gig work, or fair work practices in Africa.\n\"\"\"\n\n--- END OF FILE: utils/prompts.py ---\n\n--- START OF FILE: utils/embeddings.py ---\n\n# import sentence_transformers\n\n# # model = sentence_transformers.SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\", trust_remote_code=True)\n\n\n# def embed_text(text: str) -> list[float]:\n#     \"\"\"\n#     Embed a given text using the Nomic AI embedding model.\n\n#     Args:\n#         text (str): The text to embed.\n\n#     Returns:\n#         list[float]: The embedding vector for the text.\n#     \"\"\"\n#     return model.encode(text, convert_to_tensor=True).tolist()\n\n--- END OF FILE: utils/embeddings.py ---\n\n--- START OF FILE: utils/chuck_document.py ---\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n\ndef chunk_document(document: str, chunk_size: int = 768, chunk_overlap: int = 200) -> list[str]:\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    return text_splitter.split_text(document)\n\n--- END OF FILE: utils/chuck_document.py ---\n\n--- START OF FILE: utils/data_extraction.py ---\n\nimport json\nfrom typing import Dict, Any\nimport asyncio\nfrom pydantic import BaseModel\nfrom llama_index.core import PromptTemplate\nfrom schemas.query_schemas import DocumentMetadata\n\nMETADATA_EXTRACTION_PROMPT = \"\"\"\nAnalyze the following user query to identify any specific constraints related to document metadata.\nExtract values for the following fields if mentioned:\n- year (as a number, e.g., 2023)\n- country (as a string, e.g., Germany)\n- document_type (as a string, e.g., Legal Framework, report, Policy Brief, Legal Paper)\n- sub_topic (as a string)\n- title (as a string)\n- sub_category (as a string)\n\nIgnore fields not mentioned or unclear values.\nIf a field is mentioned, provide its value. If multiple values for the same field are mentioned (e.g., \"reports from 2022 or 2023\"), try to extract the first clear value or indicate ambiguity if necessary (though simple equality is assumed).\nIf no metadata constraints are found, return an empty JSON object.\n\nOutput the extracted metadata as a JSON object. Do NOT include any other text or formatting outside the JSON.\n\nExample Queries and Expected Output:\nQuery: Tell me about reports from 2022 in Germany.\nOutput: {{\"year\": 2022, \"country\": \"Germany\", \"document_type\": \"report\"}}\n\nQuery: What policies were released in 2023?\nOutput: {{\"year\": 2023, \"document_type\": \"policy\"}}\n\nQuery: Find documents about sustainable agriculture.\nOutput: {{}}\n\nQuery: Show me the document titled 'Annual Report'.\nOutput: {{\"title\": \"Annual Report\"}}\n\nQuery: {query_str}\nOutput:\n\"\"\"\n\nmetadata_extraction_template = PromptTemplate(METADATA_EXTRACTION_PROMPT)\n\nasync def extract_metadata_filters_from_query(query_text: str, llm, logger) -> Dict[str, Any]:\n    \"\"\"Uses LLM to extract metadata filters from a natural language query.\"\"\"\n    logger.info(f\"Attempting to extract metadata filters from query: '{query_text}'\")\n    try:\n        # Use the configured LLM to generate the extraction prompt\n        prompt = metadata_extraction_template.format(query_str=query_text)\n\n        # Call the LLM\n        response = llm.complete(prompt)\n        llm_output = response.text.strip()\n\n        logger.info(f\"LLM extraction raw output: {llm_output}\")\n\n        # Attempt to parse the JSON output\n        # Sometimes LLMs might add extra text, try to find the JSON part\n        try:\n            json_start = llm_output.find('{')\n            json_end = llm_output.rfind('}') + 1\n            if json_start != -1 and json_end != -1:\n                 json_string = llm_output[json_start:json_end]\n                 extracted_filters = json.loads(json_string)\n            else:\n                 logger.warning(\"Could not find JSON object in LLM output.\")\n                 extracted_filters = {} # Return empty if no JSON found\n\n\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON from LLM output: {e}. Output: {llm_output}\", exc_info=True)\n            extracted_filters = {} # Return empty on JSON error\n        except Exception as e:\n            logger.error(f\"Unexpected error parsing LLM output: {e}. Output: {llm_output}\", exc_info=True)\n            extracted_filters = {} # Return empty on other parsing errors\n\n        # Basic validation against the DocumentMetadata model keys\n        valid_filters = {}\n        allowed_keys = DocumentMetadata.__fields__.keys()\n        for key, value in extracted_filters.items():\n            if key in allowed_keys and value is not None: # Only include keys defined in our model\n                 valid_filters[key] = value\n            else:\n                logger.warning(f\"LLM extracted potentially invalid filter key/value: {key}: {value}. Ignoring.\")\n\n        logger.info(f\"Extracted and validated filters: {valid_filters}\")\n        return valid_filters\n\n    except Exception as e:\n        logger.error(f\"An error occurred during LLM metadata extraction: {e}\", exc_info=True)\n        return {} # Return empty filters on any error\n\n\n--- END OF FILE: utils/data_extraction.py ---\n\n--- START OF FILE: utils/document_reader.py ---\n\nfrom langchain_community.document_loaders import PyPDFLoader, UnstructuredPowerPointLoader, Docx2txtLoader\n\ndef read_pdf(file_path: str) -> str:\n    \"\"\"\n    Reads a PDF file and extracts text from it.\n    \"\"\"\n    loader = PyPDFLoader(file_path)\n    documents = loader.load()\n    return \"\\n\".join([doc.page_content for doc in documents])\n\ndef read_powerpoint(file_path: str) -> str:\n    \"\"\"\n    Reads a PowerPoint file and extracts text from it.\n    \"\"\"\n    loader = UnstructuredPowerPointLoader(file_path)\n    documents = loader.load()\n    return \"\\n\".join([doc.page_content for doc in documents])\n\ndef read_docx(file_path: str) -> str:\n    \"\"\"\n    Reads a DOCX file and extracts text from it.\n    \"\"\"\n    loader = Docx2txtLoader(file_path)\n    documents = loader.load()\n    return \"\\n\".join([doc.page_content for doc in documents])\n\n\n--- END OF FILE: utils/document_reader.py ---\n\n--- START OF FILE: models/file_tables.py ---\n\nfrom datetime import datetime\nfrom sqlmodel import Field, SQLModel\n\nclass FileTable(SQLModel, table=True):\n    id: int = Field(default=None, primary_key=True)\n    file_title: str\n    file_path: str | None = Field(default=None)\n    file_source: str\n    uploaded_at: datetime = Field(default_factory=datetime.utcnow)\n\n--- END OF FILE: models/file_tables.py ---\n\n--- START OF FILE: models/documents_table.py ---\n\nfrom datetime import datetime\nfrom sqlmodel import Field, SQLModel\n\nclass DocumentTable(SQLModel, table=True):\n    id: int = Field(default=None, primary_key=True)\n    document_str_id: str\n    file_table_id: int = Field(foreign_key=\"filetable.id\")\n\n--- END OF FILE: models/documents_table.py ---\n\n--- START OF FILE: schemas/query_schemas.py ---\n\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass QueryRequest(BaseModel):\n    query: str\n    \n    \nclass DocumentMetadata(BaseModel):\n    title: str\n    year: Optional[int] = None\n    document_type: Optional[str] = None\n    sub_topic: Optional[str] = None\n    title: Optional[str] = None\n    sub_category: Optional[str] = None\n    country: Optional[str] = None\n    file_source: Optional[str] = None\n    \nclass Source(BaseModel):\n    file_name: str\n    page_label: int\n    document_metadata: DocumentMetadata\n\n--- END OF FILE: schemas/query_schemas.py ---\n\n--- START OF FILE: api/v1/model.py ---\n\nfrom fastapi import APIRouter, Depends, Request, UploadFile\nfrom fastapi.responses import JSONResponse\nimport os\nimport uuid\nfrom schemas.query_schemas import QueryRequest, Source, DocumentMetadata\nimport json\nfrom knowledge_base.core import logger\nfrom pydantic import ValidationError\nimport asyncio\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.readers.web import SimpleWebPageReader\nfrom llama_index.core.schema import Document, NodeWithScore\nfrom database.db import engine\nfrom sqlmodel import SQLModel, Session\nfrom models.file_tables import FileTable\nfrom models.documents_table import DocumentTable\nfrom typing import Annotated\nfrom knowledge_base.langgraph_init import agent_graph\n\nrouter = APIRouter()\n\ndef get_session():\n    with Session(engine) as session:\n        yield session\n        \nSessionDep = Annotated[Session, Depends(get_session)]\n\nUPLOADED_FILES_DIR = \"uploaded_files\"  # Directory to save uploaded files\n\nos.makedirs(UPLOADED_FILES_DIR, exist_ok=True)  # Ensure the directory exists\n\n# export the agent graph to png\n\nprint(agent_graph.get_graph().draw_mermaid())\n\n@router.post(\"/add_knowledge\")\nasync def add_file_knowledge(\n    request: Request, \n    session: SessionDep,\n    file: Optional[UploadFile] = None, \n):\n    \"\"\"\n    Endpoint to add knowledge to the knowledge base.\n    File upload is optional.\n    \"\"\"\n    try:\n        # Reads all fields from the request\n        form_data = await request.form()\n        documents_to_update: List[Document] = []\n        document_ids = set()\n        uploaded_file_path = None\n\n        # Save the uploaded file if provided\n        print(f\"Received file: {file.filename if file else 'None'}\")\n        if file and file.filename:\n            uploaded_file_path = Path(UPLOADED_FILES_DIR) / f\"{uuid.uuid4()}_{file.filename}\"\n            try:\n                contents = await file.read()\n                await asyncio.to_thread(uploaded_file_path.write_bytes, contents)\n                logger.info(f\"File saved to {uploaded_file_path}\")\n            except Exception as e:\n                error_msg = f\"Failed to save file {file.filename}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                return JSONResponse(content={\"status\": \"error\", \"message\": error_msg}, status_code=500)\n\n        # Check if title is provided in the form data\n        if \"title\" not in form_data or not form_data.get(\"title\"):\n            return JSONResponse(content={\"status\": \"error\", \"message\": \"Title is required\"}, status_code=400)\n        \n        # Check if the file_source is provided in the form data\n        if \"file_source\" not in form_data or not form_data.get(\"file_source\"):\n            return JSONResponse(content={\"status\": \"error\", \"message\": \"File source is required\"}, status_code=400)\n        \n        metadata_dict = {\n            \"title\": form_data.get(\"title\"),\n            \"file_source\": form_data.get(\"file_source\")\n        }\n        \n        for field in [\"year\", \"document_type\", \"sub_topic\", \"title\", \"sub_category\", \"country\", \"file_source\"]:\n            if field in form_data and form_data.get(field):\n                if field == \"year\" and form_data.get(field).isdigit():\n                    metadata_dict[field] = int(form_data.get(field))\n                else:\n                    metadata_dict[field] = form_data.get(field)\n        \n        document_metadata = DocumentMetadata(**metadata_dict)\n        \n        extra_metadata_dict = {}\n        \n        if document_metadata:\n            try:\n                extra_metadata_dict = document_metadata.model_dump(exclude_none=True) \n                logger.info(f\"Parsed and validated metadata: {extra_metadata_dict}\")\n            except json.JSONDecodeError:\n                error_msg = \"Metadata is not a valid JSON string.\"\n                logger.error(error_msg)\n            except ValidationError as e:\n                error_msg = f\"Metadata validation failed: {e.errors()}\"\n                logger.error(error_msg)\n            except Exception as e:\n                error_msg = f\"Unexpected error parsing metadata: {e}\"\n                logger.error(error_msg, exc_info=True)\n        else:\n            logger.info(\"No metadata provided for this file.\")\n            \n        # Load documents if a file was uploaded\n        if uploaded_file_path:\n            try:\n                docs = SimpleDirectoryReader(input_files=[str(uploaded_file_path)]).load_data()\n                if docs:\n                    for doc in docs:\n                        # Add the extra metadata to the document's metadata dictionary\n                        doc.metadata.update(extra_metadata_dict)\n                        documents_to_update.append(doc)\n                        document_ids.add(doc.doc_id)\n                        \n                    logger.info(f\"Loaded {len(documents_to_update)} document(s) from {uploaded_file_path} with added metadata.\")\n                else:\n                    warning_msg = f\"No documents loaded from file {file.filename}. Skipping index update.\"\n                    logger.warning(warning_msg)\n            except Exception as e:\n                error_msg = f\"Failed to load document from {uploaded_file_path}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                return JSONResponse(content={\"status\": \"error\", \"message\": error_msg}, status_code=500)\n            \n        else:\n            # Use the web reader to fetch documents from the file_source provided in the form data\n            file_source = form_data.get(\"file_source\")\n            try:\n                logger.info(f\"Fetching documents from web source: {file_source}\")\n                docs = SimpleWebPageReader(html_to_text=True).load_data(\n                    [file_source]\n                )\n                if docs:\n                    for doc in docs:\n                        # Add the extra metadata to the document's metadata dictionary\n                        doc.metadata.update(extra_metadata_dict)\n                        documents_to_update.append(doc)\n                        document_ids.add(doc.doc_id)\n                        \n                    logger.info(f\"Loaded {len(documents_to_update)} document(s) from web source {file_source} with added metadata.\")\n                else:\n                    warning_msg = f\"No documents loaded from web source {file_source}. Skipping index update.\"\n                    logger.warning(warning_msg)\n            except Exception as e:\n                error_msg = f\"Failed to fetch documents from web source {file_source}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                return JSONResponse(content={\"status\": \"error\", \"message\": error_msg}, status_code=500)\n\n        # Save the metadata to the database\n        try:\n            print(document_ids)\n            print(uploaded_file_path)\n            # Save the documents to the documents table and the file to the files table\n            \n            file_record = FileTable(\n                file_title=form_data.get(\"title\"),\n                file_path=str(uploaded_file_path) if uploaded_file_path else None,\n                file_source=form_data.get(\"file_source\")\n            )\n\n            session.add(file_record)\n            session.commit()\n\n            for doc_id in document_ids:\n                document_record = DocumentTable(\n                    document_str_id=doc_id,\n                    file_table_id=file_record.id\n                )\n                session.add(document_record)\n\n            session.commit()\n            logger.info(f\"Metadata saved to database with ID {file_record.id}.\")\n        except Exception as e:\n            error_msg = f\"Failed to save metadata to database: {e}\"\n            logger.error(error_msg, exc_info=True)\n            return JSONResponse(content={\"status\": \"error\", \"message\": error_msg}, status_code=500)\n\n        # Update the index with the new document\n        index = request.app.state.llama_index\n        \n        try:\n            logger.info(\"Refreshing index with document(s) and metadata...\")\n            actually_updated_docs = index.refresh_ref_docs(\n                documents=documents_to_update,\n                service_context=None\n            )\n            \n            logger.info(f\"Index refresh complete. {len(actually_updated_docs)} document(s) were added or refreshed.\")\n            \n        except Exception as e:\n            error_msg = f\"An error occurred during index refresh: {e}\"\n            logger.error(error_msg, exc_info=True)\n            return JSONResponse(content={\"status\": \"error\", \"message\": error_msg}, status_code=500)\n        \n        # Persist the updated index\n        try:\n            index.storage_context.persist()\n        except Exception as e:\n            error_msg = f\"Failed to persist index: {e}\"\n            logger.error(error_msg, exc_info=True)\n            return JSONResponse(content={\"status\": \"error\", \"message\": error_msg}, status_code=500)\n\n        return JSONResponse(content={\"status\": \"success\", \"message\": \"Knowledge added successfully\"}, status_code=200)\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return JSONResponse(content={\"status\": \"error\", \"message\": str(e)}, status_code=500)\n\n\n\n# New Endpoint to query the knowledge base\n@router.post(\"/query\")\nasync def query_knowledge(request: Request, query_request: QueryRequest):\n    \"\"\"\n    Endpoint to query the knowledge base.\n    Extracts metadata filters from the query and uses them in the vector store query.\n    Returns the response and source documents.\n    \"\"\"\n    try:\n        query_text = query_request.query\n        logger.info(f\"Received query: '{query_text}'\")\n        \n        # The inputs for the graph\n        inputs = {\"messages\": [(\"user\", query_text)]}\n\n        # Get the LlamaIndex instance from the app state\n        index_instance = request.app.state.llama_index\n        \n        # The runtime configuration, injecting the LlamaIndex instance\n        config = {\"configurable\": {\"llama_index\": index_instance}}\n\n        # Asynchronously invoke the agent graph\n        final_state = await agent_graph.ainvoke(inputs, config=config)\n        \n        # Extract the final AI message\n        response = final_state['messages'][-1].content\n        \n        return JSONResponse(content={\n            \"response\": response\n        }, status_code=200)\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        logger.error(f\"An error occurred during query processing: {e}\", exc_info=True)\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"An internal error occurred during the query.\"}, status_code=500)\n    \n\n# Get all the uploaded documents/files\n@router.get(\"/files\", response_model=List[FileTable])\nasync def get_uploaded_files(session: SessionDep):\n    \"\"\"\n    Get all uploaded files from the database.\n    \"\"\"\n    try:\n        files = session.query(FileTable).all()\n        return files\n    except Exception as e:\n        logger.error(f\"Error fetching uploaded files: {e}\", exc_info=True)\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"Failed to fetch uploaded files\"}, status_code=500)\n    \n\n# Delete a specific file by ID\n@router.delete(\"/delete_file/{file_id}\")\nasync def delete_file(request : Request, file_id: int, session: SessionDep):\n    \"\"\"\n    Delete a specific file by ID.\n    \"\"\"\n    try:\n        file_record = session.get(FileTable, file_id)\n        if not file_record:\n            return JSONResponse(content={\"status\": \"error\", \"message\": \"File not found\"}, status_code=404)\n        \n        index = request.app.state.llama_index\n        # Remove the document from the index\n        logger.info(f\"Attempting to delete file with ID {file_id} and associated documents from index...\")\n\n        all_document_ids = session.query(DocumentTable).filter(DocumentTable.file_table_id == file_record.id).all()\n        if not all_document_ids:\n            logger.warning(f\"No associated documents found for file ID {file_id}.\")\n            return JSONResponse(content={\"status\": \"warning\", \"message\": \"No associated documents found for this file\"}, status_code=404)\n\n        for doc in all_document_ids:\n            try:\n                # Remove each document ID from the index\n                logger.info(f\"Removing document with ID {doc.id} from index...\")\n                index.delete_ref_doc(doc.id, delete_from_docstore=True)\n                logger.info(f\"Document with ID {doc.id} removed from index.\")\n            except Exception as e:\n                logger.error(f\"Failed to remove document with ID {doc.id} from index: {e}\", exc_info=True)\n                return JSONResponse(content={\"status\": \"error\", \"message\": \"Failed to remove document from index\"}, status_code=500)\n\n        # Remove the file from the filesystem\n        if file_record.file_path and os.path.exists(file_record.file_path):\n            os.remove(file_record.file_path)\n            logger.info(f\"File {file_record.file_path} deleted successfully.\")\n\n        # Remove the associated documents from the database\n        session.query(DocumentTable).filter(DocumentTable.file_table_id == file_record.id).delete()\n        session.commit()\n        \n        # Delete the record from the database\n        session.delete(file_record)\n        session.commit()\n        \n        return JSONResponse(content={\"status\": \"success\", \"message\": \"File deleted successfully\"}, status_code=200)\n    \n    except Exception as e:\n        logger.error(f\"Error deleting file: {e}\", exc_info=True)\n        return JSONResponse(content={\"status\": \"error\", \"message\": \"Failed to delete file\"}, status_code=500)\n    \n\n\n--- END OF FILE: api/v1/model.py ---\n\n--- START OF FILE: knowledge_base/setup.py ---\n\n\nfrom fastapi import FastAPI, Request, HTTPException, UploadFile, File, Depends\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport contextlib\nfrom .core import setup_llama_index_components, logger, INDEX_KEY, QUERY_ENGINE_KEY\nfrom database.db import engine\nfrom models.file_tables import *\nfrom models.documents_table import *\nfrom sqlmodel import SQLModel\n\n# --- FastAPI Lifespan ---\n\n@contextlib.asynccontextmanager\nasync def lifespan_event_handler(app: FastAPI):\n    \"\"\"\n    FastAPI lifespan. Sets up LlamaIndex and stores index/query engine in app state.\n    \"\"\"\n    print(\"Application startup begins.\")\n\n    # --- Startup: Setup LlamaIndex and Query Engine ---\n    index = None\n    vector_store = None\n    try:\n        index, vector_store = setup_llama_index_components()\n        if index and vector_store:\n            # Store the index and vector store in the application state\n            app.state.llama_index = index\n            app.state.vector_store = vector_store \n\n            logger.info(\"Creating initial query engine...\")\n            app.state.query_engine = index.as_query_engine(similarity_top_k=3)\n            logger.info(\"Initial query engine created and stored in app state.\")\n\n        else:\n             logger.error(\"LlamaIndex setup failed. Index or vector store not available.\")\n             raise RuntimeError(\"Failed to initialize LlamaIndex components.\")\n         \n        \n        # Initialize the database engine\n        logger.info(\"Initializing database engine...\")\n        SQLModel.metadata.create_all(engine)\n        logger.info(\"Database engine initialized and tables created if not exist.\")\n         \n\n    except Exception as e:\n        logger.critical(f\"Critical error during LlamaIndex setup: {e}\", exc_info=True)\n        raise RuntimeError(\"Application startup failed due to LlamaIndex setup error.\") from e\n\n    # --- Startup successful ---\n    print(\"Application startup complete. Yielding control.\")\n    yield # Application starts serving requests now\n    \n    print(\"Application shutdown complete.\")\n\n\n\n--- END OF FILE: knowledge_base/setup.py ---\n\n--- START OF FILE: knowledge_base/langgraph_init.py ---\n\nfrom dotenv import load_dotenv\nfrom typing import Annotated, Literal, List\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\nfrom tools.agent_tools import get_information_from_db, search_internet\nfrom utils import prompts\nfrom langchain_core.runnables import RunnableConfig\n\n\nload_dotenv()\n\nllm = init_chat_model(\n    \"google_genai:gemini-2.0-flash\"\n)\n\n# --- 1. State Definition ---\nclass GraphState(TypedDict):\n    messages: Annotated[list, add_messages]\n    question: str\n    documents: List[str]\n    message_type: str\n    retrieval_status: str\n    rewritten_question: str\n\n# --- 2. Pydantic Models for LLM Decisions ---\nclass MessageClassifier(BaseModel):\n    \"\"\"Classify the user's message.\"\"\"\n    message_type: Literal[\"general_question\", \"context_needed\"] = Field(\n        description=\"The category of the user's message.\"\n    )\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for document relevance.\"\"\"\n    is_sufficient: bool = Field(description=\"Are the documents sufficient to answer the question?\")\n    reasoning: str = Field(description=\"A brief explanation for the decision.\")\n\n# --- 3. Graph Nodes ---\n\nasync def classify_message(state: GraphState):\n    last_message = state[\"messages\"][-1]\n    question = last_message.content\n    classifier = llm.with_structured_output(MessageClassifier)\n    \n    result = await classifier.ainvoke([\n        {\"role\": \"system\", \"content\": prompts.CLASSIFIER_PROMPT},\n        {\"role\": \"user\", \"content\": question}\n    ])\n    \n    return {\"message_type\": result.message_type, \"question\": question}\n\nasync def general_agent(state: GraphState):\n    response = await llm.ainvoke([\n        {\"role\": \"system\", \"content\": prompts.GENERAL_BOT_PROMPT},\n        {\"role\": \"user\", \"content\": state[\"question\"]}\n    ])\n    return {\"messages\": [response]}\n\nasync def retrieve_from_db(state: GraphState, config: RunnableConfig):\n    question = state[\"question\"]\n    index = config[\"configurable\"][\"llama_index\"]\n    \n    docs = await get_information_from_db(query_text=question, index=index)\n    print(docs)\n\n    formatted_docs = [f\"Title: {doc['metadata'].get('title', 'N/A')} \\nSource: {doc['metadata'].get('file_source', 'N/A')}\\nContent: {doc['text']}\" for doc in docs]\n    return {\"documents\": formatted_docs}\n\nasync def grade_retrieval(state: GraphState):\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    \n    if not documents:\n        return {\"retrieval_status\": \"insufficient\"}\n\n    grader = llm.with_structured_output(GradeDocuments)\n    task = f\"User Question: {question}\\n\\nRetrieved Documents:\\n{''.join(documents)}\"\n    result = await grader.ainvoke([\n        {\"role\": \"system\", \"content\": prompts.GRADER_PROMPT},\n        {\"role\": \"user\", \"content\": task}\n    ])\n    \n    status = \"sufficient\" if result.is_sufficient else \"insufficient\"\n    return {\"retrieval_status\": status}\n\nasync def rewrite_query(state: GraphState):\n    \"\"\"\n    Rewrites the user's question into an effective search query.\n    \"\"\"\n    print(\"--- Rewriting Query for Web Search ---\")\n    question = state[\"question\"]\n    \n    prompt = prompts.REWRITE_QUERY_PROMPT.format(question=question)\n    \n    # Use the LLM to get the rewritten query\n    rewriter_llm = llm.with_config(tags=[\"rewriter\"])\n    rewritten_query = await rewriter_llm.ainvoke(prompt)\n    \n    print(f\"Original: '{question}' | Rewritten: '{rewritten_query.content}'\")\n    \n    return {\"rewritten_question\": rewritten_query.content}\n\nasync def search_the_web(state: GraphState):\n    \"\"\"\n    Performs an internet search using the REWRITTEN query.\n    \"\"\"\n    rewritten_query = state[\"rewritten_question\"]\n    web_results = await search_internet(query=rewritten_query)\n    \n    all_docs = state.get(\"documents\", []) + [web_results]\n    return {\"documents\": all_docs}\n\nasync def generate_answer(state: GraphState):\n    question = state[\"question\"]\n    context_str = \"\\n\\n\".join(state[\"documents\"])\n    \n    prompt = prompts.CONTEXTUAL_AGENT_PROMPT.format(context=context_str)\n    response = await llm.ainvoke([\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": question}\n    ])\n    return {\"messages\": [response]}\n\n\n# --- 4. Graph Definition and Compilation ---\ndef create_agent_graph():\n    \"\"\"Factory function to create the LangGraph agent.\"\"\"\n    workflow = StateGraph(GraphState)\n\n    # Add nodes (including the new one)\n    workflow.add_node(\"classifier\", classify_message)\n    workflow.add_node(\"general_agent\", general_agent)\n    workflow.add_node(\"retrieve_from_db\", retrieve_from_db)\n    workflow.add_node(\"grade_retrieval\", grade_retrieval)\n    workflow.add_node(\"rewrite_query\", rewrite_query) # <-- New node\n    workflow.add_node(\"internet_search\", search_the_web) # This is your updated search node\n    workflow.add_node(\"generate_answer\", generate_answer)\n\n    # Add edges\n    workflow.set_entry_point(\"classifier\")\n    workflow.add_conditional_edges(\n        \"classifier\",\n        lambda state: state[\"message_type\"],\n        {\"general_question\": \"general_agent\", \"context_needed\": \"retrieve_from_db\"}\n    )\n    \n    # *** THIS IS THE KEY LOGIC CHANGE ***\n    workflow.add_conditional_edges(\n        \"grade_retrieval\",\n        lambda state: state[\"retrieval_status\"],\n        {\n            \"sufficient\": \"generate_answer\",\n            \"insufficient\": \"rewrite_query\"\n        }\n    )\n    \n    workflow.add_edge(\"retrieve_from_db\", \"grade_retrieval\")\n    workflow.add_edge(\"rewrite_query\", \"internet_search\")\n    workflow.add_edge(\"internet_search\", \"generate_answer\")\n    workflow.add_edge(\"general_agent\", END)\n    workflow.add_edge(\"generate_answer\", END)\n\n    return workflow.compile()\n# Create the compiled graph once when the module is loaded\nagent_graph = create_agent_graph()\n\n--- END OF FILE: knowledge_base/langgraph_init.py ---\n\n",
  "full_usage": {
    "tokens": 11217,
    "cost": 0.32721
  },
  "compressed_usage": {
    "tokens": 11217,
    "cost": 0.32721
  },
  "comparison": {
    "savings": 0.0,
    "tokens": 0,
    "cost": 0.0,
    "carbon_saved": 0.0,
    "time_saved": 0
  }
}